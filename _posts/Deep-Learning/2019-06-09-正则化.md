---
layout: post
title: 正则化
category: 深度学习
tags: 正则化
description:
---

## Batch Normalization（批标准化）

- 加速网络的训练（缓解梯度消失，支持更大的学习率）
- 防止过拟合
- 降低了参数初始化的要求

### 动机

- 训练的本质是学习数据分布。如果训练数据与测试数据的分布不同会降低模型的泛化能力。因此，应该在开始训练前对所有输入数据做归一化处理。
- 而在神经网络中，因为每个隐层的参数不同，会使下一层的输入发生变化，从而导致每一批数据的分布也发生改变；致使网络在每次迭代中都需要拟合不同的数据分布，增大了网络的训练难度与过拟合的风险。

### 基本原理

BN 方法会针对每一批数据，在网络的每一层输入之前增加归一化处理，使输入的均值为 0，标准差为 1。目的是将数据限制在统一的分布下。

具体来说，针对每层的第 k 个神经元，计算这一批数据在第 k 个神经元的均值与标准差，然后将归一化后的值作为该神经元的激活值。

