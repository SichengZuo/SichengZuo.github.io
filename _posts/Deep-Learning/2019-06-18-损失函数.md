---
layout: post
title: 损失函数
category: 深度学习
tags: 损失函数
description:
---

### LogLoss对数损失函数（逻辑回归，交叉熵损失）

有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到，而逻辑回归得到的并不是平方损失。在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数（即max F(y, f(x)) —> min -F(y, f(x)))。从损失函数的视角来看，它就成了log损失函数了。

log损失函数的标准形式：

$$
L ( Y , P ( Y | X ) ) = - \log P ( Y | X )
$$

刚刚说到，取对数是为了方便计算极大似然估计，因为在MLE（最大似然估计）中，直接求导比较困难，所以通常都是先取对数再求导找极值点。损失函数L(Y, P(Y|X))表达的是样本X在分类Y的情况下，使概率P(Y|X)达到最大值（换言之，就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大）。因为log函数是单调递增的，所以logP(Y|X)也会达到最大值，因此在前面加上负号之后，最大化P(Y|X)就等价于最小化L了。

逻辑回归的P(Y=y|x)表达式如下（为了将类别标签y统一为1和0，下面将表达式分开表示）：

$$
P ( Y = y | x ) = \left\{ \begin{array} { c l } { h _ { \theta } ( x ) = g ( f ( x ) ) = \frac { 1 } { 1 + \exp \{ - f ( x ) \} } } & { , y = 1 } \\ { 1 - h _ { \theta } ( x ) = 1 - g ( f ( x ) ) = \frac { 1 } { 1 + \exp f ( x ) \} } } & { , y = 0 } \end{array} \right.
$$

逻辑回归最后得到的目标式子如下：

$$
J ( \theta ) = - \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left[ y ^ { ( i ) } \log h _ { \theta } \left( x ^ { ( i ) } \right) + \left( 1 - y ^ { ( i ) } \right) \log \left( 1 - h _ { \theta } \left( x ^ { ( i ) } \right) \right) \right]
$$

上面是针对二分类而言的。这里需要解释一下：之所以有人认为逻辑回归是平方损失，是因为在使用梯度下降来求最优解的时候，它的迭代式子与平方损失求导后的式子非常相似，从而给人一种直观上的错觉。

**注意：softmax使用的即为交叉熵损失函数，binary_cossentropy为二分类交叉熵损失，categorical_crossentropy为多分类交叉熵损失，当使用多分类交叉熵损失函数时，标签应该为多分类模式，即使用one-hot编码的向量。**

### 平方损失函数（最小二乘法, Ordinary Least Squares ）

最小二乘法是线性回归的一种，最小二乘法（OLS）将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是**中心极限定理**，最后通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：**最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小**。换言之，OLS是基于距离的，而这个距离就是我们用的最多的**欧几里得距离**。为什么它会选择使用欧式距离作为误差度量呢（即Mean squared error， MSE），主要有以下几个原因：

- 简单，计算方便；
- 欧氏距离是一种很好的相似性度量标准；
- 在不同的表示域变换后特征性质不变。

平方损失（Square loss）的标准形式如下：

$$
L ( Y , f ( X ) ) = ( Y - f ( X ) ) ^ { 2 }
$$

当样本个数为n时，此时的损失函数变为：

$$
L ( Y , f ( X ) ) = \sum _ { i = 1 } ^ { n } ( Y - f ( X ) ) ^ { 2 }
$$

`Y-f(X)`表示的是残差，整个式子表示的是**残差的平方和**，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是**最小化残差的平方和（residual sum of squares，RSS）**。

而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标，公式如下：

$$
M S E = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \left( \tilde { Y } _ { i } - Y _ { i } \right) ^ { 2 }
$$

上面提到了线性回归，这里额外补充一句，我们通常说的线性有两种情况，一种是因变量y是自变量x的线性函数，一种是因变量y是参数。

### 指数损失函数（Adaboost）

学过Adaboost算法的人都知道，它是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。在Adaboost中，经过m此迭代之后，可以得到

$$
f _ { m } ( x ) = f _ { m - 1 } ( x ) + \alpha _ { m } G _ { m } ( x )
$$

Adaboost每次迭代时的目的是为了找到最小化下列式子时的参数

$$
\arg \min _ { \alpha , G } = \sum _ { i = 1 } ^ { N } \exp \left[ - y _ { i } \left( f _ { m - 1 } \left( x _ { i } \right) + \alpha G \left( x _ { i } \right) \right) \right]
$$

而指数损失函数（exp-loss）的标准形式如下

$$
L ( y , f ( x ) ) = \exp [ - y f ( x ) ]
$$

可以看出，Adaboost的目标式子就是指数损失，在给定n个样本的情况下，Adaboost的损失函数为：

$$
L ( y , f ( x ) ) = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \exp \left[ - y _ { i } f \left( x _ { i } \right) \right]
$$

### Hinge损失函数（SVM）

