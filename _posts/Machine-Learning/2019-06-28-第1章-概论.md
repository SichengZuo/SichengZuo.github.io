---
layout: post
title: 统计学习方法-第1章-绪论
category: 机器学习
tags: 监督学习
keywords: 监督学习
description:
---

## 统计学习分类

|分类标准|类型|
|:---:|:---:|
|基本分类|监督学习、无监督学习、强化学习|
|按模型分类|概率模型、非概率模型<br>（在监督学习中，概率模型是生成模型，非概率模型是判别模型）|
|按算法分类|在线学习、批量学习|
|按技巧分类|贝叶斯学习、核方法|

## 统计学习方法三要素

### 模型

在监督学习过程中，模型就是所要学习的**条件概率分布**或者**决策函数**。

||假设空间$\mathcal { F }$ | 输入空间 $\mathcal { X }$ | 输出空间 $\mathcal { Y }$ |参数空间|
|----|-----|----|----|----|
|决策函数|$$\mathcal { F } = \left\{ f _ { \theta } \mid Y = f _ { \theta } ( x ) , \theta \in \mathbf { R } ^ { n } \right\}$$ |变量|变量| $$\mathbf { R } ^ { n }$$ |
|条件概率分布| $$\mathcal { F } = \left\{ P \mid P _ { \theta } ( Y \mid X ) , \theta \in \mathbf { R } ^ { n } \right\}$$ |随机变量|随机变量| $$\mathbf { R } ^ { n }$$ |

### 策略

#### 损失函数与风险函数

**损失函数**度量模型**一次预测**的好坏，**风险函数**度量**平均意义**下模型预测的好坏。

1. 损失函数(loss function)或代价函数(cost function)

    损失函数定义为给定输入$X$的**预测值$f(X)$**和**真实值$Y$**之间的**非负实值**函数，记作$L(Y,f(X))$

2. 风险函数(risk function)或期望损失(expected loss)

    这个和模型的泛化误差的形式是一样的

    $$
    R _ { \exp } ( f ) = E _ { p } [ L ( Y , f ( X ) ) ] = \int _ { \mathcal { X } \times \mathcal { Y } } L ( y , f ( x ) ) P ( x , y ) \mathrm { d } x \mathrm { d } y
    $$

    模型 $f(X)$ 关于联合分布 $P(X,Y)$ 的**平均意义下的**损失(**期望**损失)，但是因为$P(X,Y)$是未知的，所以前面的用词是**期望**，以及**平均意义下的**。

    这个表示其实就是损失的均值，反映了对整个数据的预测效果的好坏，$P(x,y)$转换成$\frac {\nu(X=x, Y=y)} {N}$。

3. **经验风险**(empirical risk)或**经验损失**(empirical loss)

    $$
    R _ { e m p } ( f ) = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } L \left( y _ { i } , f \left( x _ { i } \right) \right)
    $$

    模型 $f$ 关于**训练样本集**的平均损失。根据大数定律，当样本容量 N 趋于无穷大时，经验风险趋于期望风险

4. **结构风险**(structural risk)

    $$
    R _ { s r m } ( f ) = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } L \left( y _ { i } , f \left( x _ { i } \right) \right) + \lambda J ( f )
    $$

    $J(f)$ 为模型复杂度, $\lambda \geqslant 0$是系数，用以权衡经验风险和模型复杂度。

#### 常用损失函数

- 0-1损失

<div>
$$
L ( Y , f ( X ) ) = \left\{ \begin{array} { l } { 1 , Y \neq f ( X ) } \\ { 0 , Y = f ( X ) } \end{array} \right.
$$
</div>

- 平方损失

$$
L ( Y , f ( X ) ) = ( Y - f ( X ) ) ^ { 2 }
$$

- 绝对损失
$$
L ( Y , f ( X ) ) = | Y - f ( X ) |
$$

- 对数损失

$$
L ( Y , P ( Y | X ) ) = - \log P ( Y | X )
$$

#### ERM 与 SRM

经验风险最小化(ERM)与结构风险最小化(SRM)

1. **极大似然估计**是经验风险最小化的一个例子。
    
    当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化等价于极大似然估计

2. **贝叶斯估计**中的**最大后验概率估计**是结构风险最小化的一个例子。
    
    当模型是条件概率分布，损失函数是对数损失函数，**模型复杂度由模型的先验概率表示**时，结构风险最小化等价于最大后验概率估计。

### 算法


## 模型评估与模型选择

- 训练误差和测试误差是模型关于数据集的平均损失。

- 统计学习方法具体采用的损失函数未必是评估时使用的损失函数。

## 正则化与交叉验证

#### 正则化

模型选择的典型方法是正则化

#### 交叉验证

另一种常用的模型选择方法是交叉验证

- 简单
- S折(K折, K-Fold)
- 留一法

## 泛化能力

- 采用最多的方法是通过测试误差来评价学习方法的泛化能力

- 统计学习理论试图从理论上对学习方法的泛化能力进行分析

- 学习方法的泛化能力往往是通过研究泛化误差的**概率上界**进行的, 简称为泛化误差上界(generalization error bound)

**事实上，泛化误差就是所学习到的模型的期望风险**

## 生成模型与判别模型

**监督学习方法**可分为**生成方法**(generative approach)与**判别方法**(discriminative approach)。

||生成方法|判别方法|
|:---:|:---|:---|
|定义|由数据学习联合概率分布 $P(X, Y)$，然后求出条件概率分布 $P(Y \mid X)$ 作为预测的模型，即生成模型 $P(Y \mid X) = \frac { P(X, Y) } { P(X) }$|由数据直接学习决策函数 $f(X)$或者条件概率分布 $P(Y \mid X)$ 作为预测的模型|
|特点|1. 可以还原出联合概率分布 P(X, Y)；<br>2. 学习收敛速度更快；<br>3. 存在隐变量时，仍可以使用；|1. 直接面对预测，学习的准确率更高<br>2. 可以对数据进行各种程度的抽象, 定义特征并使用特征, 可以简化学习问题|
|对应模型|**生成模型：**<br>朴素贝叶斯法、隐马尔可夫模型|**判别模型：**<br>k近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法、条件随机场|

## 分类问题、标注问题、回归问题

#### 分类问题

常用的统计学习方法

- k近邻法
- 感知机
- 朴素贝叶斯法
- 决策树
- 决策列表
- 逻辑斯蒂回归模型
- 支持向量机
- 提升方法
- 贝叶斯网络
- 神经网络
- Winnow

#### 标注问题

常用的统计学习方法

- 隐马尔可夫模型
- 条件随机场

#### 回归问题