---
layout: post
title: R-CNN 笔记
category: Detection
tags: 目标检测
keywords: R-CNN CVPR'14 A
description:利用CNN实现检测任务的经典算法，有突破性的改进
---

## 1. 基础框架

### R-CNN

> Title: Rich feature hierarchies for accurate object detection and semantic segmentation  
>
> Source: [CVPR' 14] 
>
> Author: Ross Girshick（UCB）
>
> Score: A

*评A原因：利用CNN实现检测任务的经典算法，有突破性的改进*

#### 摘要

此前的detection任务中，都是采用复杂的集成模型来达到最好的性能。本文将经典机器学习方法（SVM）和深度学习（CNN）相结合，提出了一种简洁的目标检测算法：从图片中提取region proposal，用CNN提取出proposal的feature，最后用SVM进行分类（物体类别or背景）。算法称为R-CNN（Regions with CNN features）。

#### 模型

从Image提取出region proposal：经典方法selective search

从proposal中提取特征的CNN模型：5层卷积层+2层全连接层（不含最后的线性分类层）（AlexNet）

根据feature对proposal进行分类的SVM：线性SVM模型，针对每个类别学习一个SVM模型

#### 算法

##### 训练过程

1. 首先将CNN模型在ImageNet上进行分类任务的预训练。
2. 然后将CNN模型在检测数据集上进行finetune。需要将CNN最后的预训练线性分类层替换为随机初始化的N+1线性分类曾（N个物体类别+背景）；用selective search方法获得proposal，并进行样本标签划分（与gt_bbox的Iou>0.5的proposal被认为是该类别的正样本，其他proposal被认为是负样本）；用分类目标finetune CNN，每个batch由随机采样、比例固定的正负proposal构成。
3. 最后用CNN获取的feature来训练一组SVM。针对每个类别都训练一个线性SVM，正样本为对应类别的gt_bbox，负样本为与gt_bbox的Iou<0.3的proposal。

##### 测试过程

1. 首先用selective search生成约2k个proposal。
2. 将每个proposal reshape为固定形状，并用CNN提取feature。
3. 将feature喂入每个类别的SVM，得到该proposal在不同类别上的概率。
4. 采用非极大值抑制，消除那些与有更高类别概率的proposal的Iou大于阈值的proposal。

#### 实验

R-CNN在VOC 2010上mAP = 50.2，在ILSVRC2013 detection上mAP = 31.4

#### 讨论

> 关于为什么要用SVM进行分类，而不直接用CNN的线性分类层：

对CNN进行finetune时，由于正样本数量远少于负样本，为了避免CNN过拟合，一些“抖动”的正样本也被用于训练（比如用Iou>0.5的样本，而不是直接用gt_bbox作为正样本）。由于在正样本的定义中没有强调位置准确的重要性，加上CNN本身具有的平移不变性，导致CNN只能很好地提取feature，而对于位置信息不够敏感，这会导致算法最终对于bbox的位置预测不够准确。

而训练SVM时，正样本是严格的gt_bbox，负样本是“hard negatives”（相比于训练CNN时随机采样的负样本），因而对于proposal的位置有更严格的约束。

这也解释了为什么finetune CNN和训练SVM时正负样本划分标准不一致。（实际上，作者是发现finetune CNN时采取上述划分策略效果更好，进而提出了以上的假设分析）

> 关于对检测结果的分析

值得一提的是，作者利用[检测错误分析工具](Diagnosing error in object detectors)分析了detection error的主要原因，发现一大原因是bbox的位置预测不够准确，进而用简单的bbox regression层进行改进。该分析工具将detection error分为以下几类原因：Loc—poor localization (a detection with an IoU overlap with the correct class between 0.1 and 0.5, or a duplicate); Sim—confusion with a similar category; Oth—confusion with a dissimilar object category; BG—a FP that fired on background.

> 关于加载预训练模型

作者推测，在有大量数据的无关任务类别上预训练，再在有少量数据的目标任务上进行finetune，将是一种高效且通用的训练范式（supervised pre-training and domain-specific finetuning）。

